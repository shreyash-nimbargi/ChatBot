Ah, gotcha! GitHub uses Markdown, and `**bold text**` just makes things bold, not a proper heading. For actual headings, you need to use `#` symbols.

Here are those "real notes" again, but formatted with proper Markdown headings so they'll look great on GitHub:

---

# Notes on Gen AI & ChatGPT

So, this video series is all about diving into Generative AI, starting with the basics. Think of it as peeling back the layers on how ChatGPT actually works. The goal is to make it make sense, even if you're not a coder â€“ just using common sense.

## ChatGPT's Core Idea: Not Stored Answers!

First big thing: ChatGPT doesn't have a giant list of pre-written answers. Seriously, imagine trying to store every possible response for infinite questions, with all the grammar mistakes and variations! Impossible.

Instead, it works a bit like how we think. If I give you a sequence like "10, 11, 12, 13," you know the next is "14" because you see a pattern. ChatGPT does that, but with words. When you type "Hi, how are you?", it looks at that and tries to predict the *next most likely word* to continue the sentence. It does this word by word until it builds a full answer.

## How Computers "Read": Tokens and Numbers

Now, computers don't understand words directly. They get numbers. So, your question ("Hi, how are you?") gets turned into "tokens," and each token is assigned a unique number. Think of it like a secret code. These numbers go into the ChatGPT model, which has been "trained" to spot patterns in these numerical sequences. Based on those patterns, it predicts the next number, which then gets translated back into a word. This keeps going until the response is complete.

## What is "Generative AI"?

"Generative AI" (that's the "Gen AI") means it can *create* totally new stuff. It's not just pulling from a database; it's generating unique answers on the fly, even if it's never seen that exact question before. That's a huge difference from older, more rigid AI systems.

## Breaking Down "GPT"

And "GPT"? That stands for "Generative Pre-trained Transformer."
* **Generative**: Like we said, it makes new things.
* **Pre-trained**: It's been fed a massive amount of data, so it understands context and can respond to related questions.
* **Transformer**: This is the type of architecture it uses to process your input and spit out the desired output, whether that's text, or even transforming text into images or video.

## Why Answers Can Vary

One cool thing: ChatGPT might give you slightly different answers each time you ask the same question. That's because it's always calculating probabilities for the next word. There isn't just *one* right answer in its world; there are many possibilities, and it picks one based on its predictive model.

So, at its core, ChatGPT is a pattern-recognizer, turning text into numbers, finding patterns, and then predicting the most probable next word to construct its unique responses.

---

This should render much better on GitHub! Let me know if you need any other Markdown adjustments.
